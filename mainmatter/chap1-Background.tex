% Chapter 1: Background
\section[Randomization in RCTs]{Randomization in clinical trials}
Research to identify effective and/or efficacious interventions requires balance between scientific and logistic constraints. 
Randomization in clinical trials allows one to infer causation from associations in the presence of an appropriate experimental design, appreciating the limitations in identifying true causal relationships. 
Complete randomization also ensures all variables, particularly those predictive of the outcome measurement (referred to as prognostic factors) have balanced distributions across groups, on average. 
However, the likelihood of imbalance increases as the trial sample size decreases or the number of measured prognostic variables increases.  

Especially in small studies, at any given point in the randomization process there could be a substantial imbalance in the number of patients assigned to each group.  
Chance imbalance in important prognostic factors may be seen as impacting the credibility of observed treatment effect estimates. 
For this reason, restricted randomization techniques are often used to guarantee the sequential and overall imbalance in the number of participants in each group is controlled.
We introduce restricted randomization and discuss their extension to control imbalance in prognostic factors by means of stratification or covariate adaptive randomization.

\section{Restricted randomization}
Blocked randomization is a widely accepted restricted randomization method that ensures current and overall imbalance in treatment assignments are controlled.  
Sequential balance is achieved by specifying a block size for which the sequence of (usually equal numbers of) treatment assignments is permuted and assigned to patients as they are enrolled. 
However, blocked randomization does not by itself guarantee overall balance in known prognostic variables of interest.  
For this reason, blocking is often combined with stratification to make study groups comparable with regard to specified stratifying factors.  

Stratified randomization is a procedure that separates the recruitment population into smaller subgroups (strata) where randomization, either simple or blocked, is performed.  
This property can be useful in multi-center trials, for instance, where it is of interest to account for between-center variability in patient outcomes due to unmeasured or unimportant factors \textit{a priori}. 
While the randomization ratio is guaranteed to hold within pre-specified blocks of enrolled subjects, in small trials with many stratification factors one cannot assure accrued patients will fill the block for each subgroup, and randomization within strata alone will not ensure balance.  

\section{Dynamic (adaptive) randomization}
Stratified block randomization is considered a static randomization method, as the probability of treatment assignment is not conditional on information on patients already enrolled.  
In contrast, adaptive (or dynamic) randomization approaches control imbalance by dynamically altering the randomization probability based on accrued patient information.  
In this thesis we consider covariate adaptive randomization procedures, which are a natural comparison to static randomization strategies intended to control imbalance of baseline prognostic factors across treatment groups.  
These procedures have been increasingly used as an alternative to stratified block randomization, particularly in small scale clinical trials with many prognostic factors.

Initial developments in covariate adaptive methods aimed to reduce the probability of undesirable, albeit unlikely, allocation sequences which result in both overall treatment group imbalances and imbalances within subgroups defined by important prognostic factors.  
In this subsection we follow the historical development of covariate adaptive approaches with a brief discussion of the characteristics and performance of a few selected methods.

Biased coin randomization introduced by Efron (1971) was the first randomization method to change the probability of assignment dynamically based on observed covariate values of accrued patients. 
Simple randomization is performed until the disparity reaches a prespecified limit, at which time the group with the least subjects is biased to have a greater probability of assignment. 

Taves (1974) extended Efron’s biased coin design to the context of small scale clinical trials, where it is of interest to constrain imbalance in multiple prognostic factors across treatment groups. 
Briefly, the method sequentially allocates incoming patients deterministically to the treatment category that minimizes the overall unweighted sum of covariate imbalance given the new assignment. 
The assignment is performed deterministically: assignment is randomized only when assignment to either treatment category results in the same imbalance. 
Pocock and Simon (1975) further generalized Taves’ method to incorporate relative importance of prognostic factors by introducing weighting of covariate imbalances into the overall imbalance metric. 

Signorini et. al 1993 extended earlier methods in order to induce balance both overall and within strata while avoiding investigator bias through unblinding.  
He proposed a tree-based method of dynamic balancing randomization (DBR) that evaluates imbalance for each prognostic factor in a nested fashion by their prespecified order of importance.  
The method flexibly allows for different levels of imbalance in different strata and ensures conditional balance, meaning that within each subgroup the ratio of treatment assignment is constrained within prespecified bounds.  
However, the method does not guarantee balanced group assignments will be achieved within each prognostic factor considered separately. 

Heritier et. al (2005) modify Signorini et. al’s DBR method to control imbalance marginally within each prognostic factor.  
For each accrued patient, the potential imbalance for each treatment assignment is considered sequentially within each prognostic factor in decreasing order of importance.  
If the potential observed imbalance exceeds a prespecified threshold, assignment is performed deterministically (or forced) to the group which minimizes the imbalance.  
Heritier et. al suggested including non-deterministic allocation to reduce the number of forced allocations and prevent investigator unblinding.

Model-based approaches are another alternative approach to dynamic randomization, where the probability of treatment assignment is chosen to minimize the variance of the estimated treatment effect. 
Model-based methods can flexibly incorporate continuous prognostic factors without the need to dichotomize into groups, and can include interaction terms and balance prognostic factors even when the number of variables is large.  
Aickin (1998, 2001, 2009) proposed a model-based approach to covariate adaptive randomization, where a subjects’ treatment assignment is based on maximizing the log-likelihood of the model. 

\section{Adjusting for prognostic factors in the analysis model}
Statistical adjustment is another approach to addressing prognostic factors that may confound the relationship between an intervention (the predictor of interest) and the outcome measure.
Oftentimes it is of interest to measure and adjust for known variables predictive of the outcome measurement in the analysis model. 
% Estimates adjusted for known prognostic factors can provide both scientific and statistical advantages.
It may be scientifically meaningful to provide estimates for known confounders or if the effect may differ within subgroups. 
Statistically, adjustment can potentially reduce bias from confounding and increase efficiency.
Statistical adjustment accounts for, but does not control, imbalance in known prognostic factors predictive of the outcome measurement. 

\section{Analysis considerations under alternative randomization schemes}
Analysis of trials using a covariate adaptive allocation (CAA) scheme must account for the randomization scheme to recover the precision gains conferred by inducing more balanced treatment groups with respect to chosen balancing factors.  
To obtain the correct variance term and significance level for the test statistic, one must consider all possible sequences of assignments which could have been made in repeated trials assuming no group differences in mean response.  
In most cases, ignoring the randomization procedure and using standard regression methods that implicitly assume complete randomization lead to larger variance estimates and conservative inference. 
Since CAA modifies the randomization scheme to induce similarity across treatment arms relative to within arms, the efficiency gain can be realized using a nonparametric re-randomization approach for estimating standard errors (Simon and Simon 2011).  
Briefly, observed values and entry order are fixed, treatment assignments are reshuffled and the test statistic computed for each permutation.  

\section{Aims}
The goal of the thesis is to address in both the binary and continuous outcome setting if covariate adaptive randomization (CAR) followed by standard asymptotic tests yield valid inference, and if so, to quantify the gains in precision relative to simple randomization (SR) or stratified block randomization (SBR).  
We will compare Heritier’s modified DBR scheme to stratified block randomization and complete randomization, while comparing re-randomization based permutation tests to standard asymptotic tests in a simulation study.  
We will consider the setting of equal allocation to treatment assignment, no temporal trend (drift), binary predictors, and two outcome types (binary and continuous).  
Our objective is to identify any scenarios, if any, where minimization improves power relative to SBR or simple randomization. 
Contour plots of effect size by sample size will compare power across methods for various outcome types and conditions. 

We are also interested if and when the answers to the above questions change when the effect size of prognostic factors is varied relative to the treatment effect, the baseline prevalence varies from 5\% to 50\% in the binary outcome setting, inference on treatment effect is performed using none (or a subset) of the prognostic variables, and when the sample size is varied.
It is well known that ignoring the minimization design tends to yield conservative inference, and that adjusting for covariates used in the randomization scheme (balancing factors) recovers type I error rate to nominal significance levels (Xu, Proschan, Lee 2016). 
Through comparison of estimated marginal and conditional treatment effects we seek to confirm this finding.
We consider different sample sizes ranging from N=32 to 96 to compare CAR to SBR as small scale trials are the setting in which alternative randomization methods are considered. 
We seek to identify the specific conditions by which CAR confers a precision advantage, if any, relative to other methods to offset the operational complexities involved in implementing an adaptive allocation procedure.  
Our intent is to provide guidance to clinical researchers for determining under what settings covariate adaptive allocation provides precision gains relative to competing approaches as well as which analysis method yields valid tests with the most power. 

Chapter 2 will introduce the notation used throughout the thesis, and Chapter 3 will discuss the design of the simulation study in further detail.  
The tables of simulation results will be presented in Chapter 4 and the key observations will be discussed in Chapter 5.

\section{Measures to evaluate aims}
For each combination of randomization scheme and analysis approach, we assess validity by estimating the nominal significance level of the test under the null hypothesis.  
We evaluate accuracy by estimating any potential bias and the coverage probability of confidence intervals, comparing those generated with standard regression methods (Wald-type) to permutation test quantile-based confidence intervals.  
Average standard error estimates will also be reported for analyses using standard regression methods.  
We evaluate efficiency by computing mean squared error (MSE) and power as a function of the true treatment effect size. 

